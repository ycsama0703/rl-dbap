# ğŸ§  Qwen2.5-7B â†’ Qwen1.8B çŸ¥è¯†è’¸é¦å®éªŒ (fdistill æ¡†æ¶)

æœ¬é¡¹ç›®å±•ç¤ºå¦‚ä½•ä½¿ç”¨ [MANGA-UOFA/fdistill](https://github.com/MANGA-UOFA/fdistill) æ¡†æ¶  
å°†ç»è¿‡ **GRPO å¾®è°ƒçš„ Qwen2.5-7B + LoRA** è’¸é¦ä¸ºæ›´è½»é‡çš„ **Qwen1.8B Student** æ¨¡å‹ã€‚  
æµç¨‹é’ˆå¯¹å•å¡ **A100-40GB** ç¯å¢ƒä¼˜åŒ–ï¼ŒåŒ…å«æ•°æ®æ··åˆã€Teacher è¾“å‡ºç”Ÿæˆä¸åœ¨çº¿è’¸é¦è®­ç»ƒã€‚

---

## ğŸ“¦ 1. ç¯å¢ƒå‡†å¤‡

```bash
conda create -n qwen_kd python=3.10 -y
conda activate qwen_kd
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install transformers==4.43.3 peft accelerate datasets bitsandbytes
pip install sentencepiece tqdm nlg-eval
```

---

## ğŸ“‚ 2. è·å– fdistill ä»“åº“

```bash
git clone https://github.com/MANGA-UOFA/fdistill.git
cd fdistill
```

---

## ğŸ§° 3. æ•°æ®å‡†å¤‡ï¼ˆ70% GRPO + 30% SFTï¼‰

```bash
mkdir -p data/mixed
python - <<'PY'
import json, random
grpo = json.load(open("data/grpo_data.json"))
sft  = json.load(open("data/sft_data.json"))
N = int(len(grpo) * 0.7)
M = int(len(sft)  * 0.3)
mix = random.sample(grpo, N) + random.sample(sft, M)
random.shuffle(mix)
json.dump(mix, open("data/mixed/train_mix.json", "w"), ensure_ascii=False, indent=2)
PY
```

---

## ğŸ§  4. ç”Ÿæˆ Teacher è¾“å‡º (ç¦»çº¿ä¼ªæ ‡ç­¾)

```bash
cat > generate_teacher_outputs.py <<'PY'
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch, json, tqdm

base_model_path = "<base model path>"
lora_path = "<checkpoint path>"
data_path = "data/mixed/train_mix.json"
output_path = "data/mixed/teacher_outputs.json"

device = "cuda"
dtype = torch.bfloat16

tokenizer = AutoTokenizer.from_pretrained(base_model_path)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path, torch_dtype=dtype, device_map="auto", load_in_8bit=True
)
model = PeftModel.from_pretrained(base_model, lora_path)
model.eval()

dataset = json.load(open(data_path))
outputs = []
for item in tqdm.tqdm(dataset):
    prompt = item.get("prompt", item.get("instruction", ""))
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        out = model.generate(**inputs, max_new_tokens=512, temperature=0.7)
    text = tokenizer.decode(out[0], skip_special_tokens=True)
    outputs.append({"prompt": prompt, "teacher_output": text})

json.dump(outputs, open(output_path, "w"), ensure_ascii=False, indent=2)
PY

python generate_teacher_outputs.py
```

è¾“å‡ºæ–‡ä»¶ï¼š`data/mixed/teacher_outputs.json`

---

## ğŸ”¥ 5. å¯åŠ¨è’¸é¦è®­ç»ƒ (KL Divergence)

```bash
cat > run_qwen_kd.sh <<'SH'
#!/bin/bash
export CUDA_VISIBLE_DEVICES=0

python train_kd.py \
  --teacher_model "<base model path>" \
  --teacher_lora "<checkpoint path>" \
  --student_model "Qwen1.8B" \
  --dataset_path "data/mixed/teacher_outputs.json" \
  --output_dir "./output/student_kd" \
  --temperature 2.0 \
  --alpha 0.5 \
  --num_train_epochs 3 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8 \
  --lr 5e-5 \
  --fp16 True \
  --teacher_8bit True \
  --gradient_checkpointing True
SH

bash run_qwen_kd.sh
```

### å‚æ•°è¯´æ˜
| å‚æ•° | å«ä¹‰ | æ¨èå€¼ |
|------|------|--------|
| `temperature` | KL Soft label å¹³æ»‘åº¦ | 2.0 |
| `alpha` | KL ä¸ CE æƒé‡ | 0.5 |
| `batch_size` | å•å¡ batch | 2 |
| `gradient_accumulation_steps` | ç´¯ç§¯æ­¥æ•° | 8 |
| `lr` | å­¦ä¹ ç‡ | 5e-5 |
| `num_train_epochs` | è®­ç»ƒè½®æ•° | 3 |

é¢„è®¡æ˜¾å­˜å ç”¨ï¼š**çº¦ 36â€“38 GB (fp16)**ã€‚

---

## ğŸ“ˆ 6. æ¨¡å‹è¯„ä¼°

```bash
python eval_model.py \
  --model_path ./output/student_kd \
  --data_path data/val.json \
  --metrics bleu rouge ppl
```

æˆ–ä½¿ç”¨ nlg-eval:
```bash
nlg-eval --hypothesis=student_output.txt --references=ref.txt
```

---

## ğŸ’¾ 7. æ˜¾å­˜å ç”¨å‚è€ƒ (A100-40GB)

| æ¨¡å¼ | è¯´æ˜ | æ˜¾å­˜ | å¤‡æ³¨ |
|------|------|------|------|
| ç”Ÿæˆä¼ªæ ‡ç­¾ | ä»… Teacher (8bit) | ~18 GB | ç¦»çº¿ç”Ÿæˆ |
| åœ¨çº¿è’¸é¦ | Teacher + Student (fp16) | ~36 GB | ä¸»è®­ç»ƒé˜¶æ®µ |
| å•æ¨¡å‹è¯„ä¼° | ä»… Student | ~20 GB | éªŒè¯é˜¶æ®µ |

---

## ğŸ“Š 8. æ¨èç›®å½•ç»“æ„

```
fdistill/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ grpo_data.json
â”‚   â”œâ”€â”€ sft_data.json
â”‚   â””â”€â”€ mixed/
â”‚       â”œâ”€â”€ train_mix.json
â”‚       â””â”€â”€ teacher_outputs.json
â”œâ”€â”€ output/
â”‚   â””â”€â”€ student_kd/
â”œâ”€â”€ generate_teacher_outputs.py
â”œâ”€â”€ run_qwen_kd.sh
â””â”€â”€ README.md
```

---

## ğŸ§© 9. å®éªŒæ€»ç»“

| é˜¶æ®µ | æ¨¡å‹ç»„åˆ | æ¨¡å¼ | ç›®æ ‡ |
|------|------------|--------|--------|
| 1ï¸âƒ£ | Qwen2.5-7B + LoRA | æ¨ç† | ç”Ÿæˆä¼ªæ ‡ç­¾ |
| 2ï¸âƒ£ | Teacher + Qwen1.8B | KL è’¸é¦ | å­¦ä¹  Teacher åˆ†å¸ƒ |
| 3ï¸âƒ£ | Qwen1.8B | è¯„ä¼° | ä¿ç•™ GRPO è¡Œä¸ºä¸é€šç”¨èƒ½åŠ› |

---

## âœ… 10. è¿è¡Œæç¤º

- è‹¥é‡ OOMï¼Œå¯è°ƒå° `batch_size` æˆ–å¢å¤§ `gradient_accumulation_steps`ã€‚  
- è‹¥ Teacher åŠ è½½æ…¢ï¼Œå¯å…ˆæ‰‹åŠ¨ merge LoRA æƒé‡è‡³ base modelã€‚  
- è‹¥ Student æ”¶æ•›æ…¢ï¼Œå¯å°† `alpha` è°ƒä½è‡³ 0.3ï¼Œå¢å¼º CE å­¦ä¹ ã€‚

---

**æœ€ç»ˆè¾“å‡ºè·¯å¾„ï¼š**
```
output/student_kd/
```
å³ä¸ºè’¸é¦å®Œæˆçš„ Qwen1.8B æ¨¡å‹ï¼Œå¯ç›´æ¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡æˆ–æ¨ç†éƒ¨ç½²ã€‚
