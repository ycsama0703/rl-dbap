import json
import math
import re
from typing import List

from swift.plugin.orm import ORM, orms

import numpy as np


ANSWER_RE = re.compile(r"<answer>\s*(.*?)\s*</answer>", re.DOTALL | re.IGNORECASE)
THINK_RE = re.compile(r"<think>.*?</think>", re.DOTALL | re.IGNORECASE)
THINK_CONTENT_RE = re.compile(r"<think>\s*(.*?)\s*</think>", re.DOTALL | re.IGNORECASE)


def _extract_answer_body(text: str) -> str | None:
    m = ANSWER_RE.search(text or "")
    if not m:
        return None
    return m.group(1).strip()


def _extract_json_from_answer(text: str):
    """Try to extract a JSON object either from <answer>...</answer> or anywhere in text.

    This makes the reward robust when the model does not emit <answer> tags yet.
    """
    def _try_load(s: str):
        try:
            return json.loads(s)
        except Exception:
            return None

    # 1) Prefer content inside <answer> tags
    body = _extract_answer_body(text)
    candidates: list[str] = []
    if isinstance(body, str) and body:
        # strip surrounding code fences if present
        cleaned = re.sub(r"^```[a-zA-Z0-9]*\n|```$", "", body.strip())
        candidates.append(cleaned)
        # substring between first { and last }
        start = cleaned.find("{")
        end = cleaned.rfind("}")
        if start != -1 and end != -1 and end > start:
            candidates.append(cleaned[start:end + 1])

    # 2) Fallback: search whole completion
    whole = text or ""
    whole_cleaned = re.sub(r"^```[a-zA-Z0-9]*\n|```$", "", whole.strip())
    start2 = whole_cleaned.find("{")
    end2 = whole_cleaned.rfind("}")
    if start2 != -1 and end2 != -1 and end2 > start2:
        candidates.append(whole_cleaned[start2:end2 + 1])

    # Try load in order
    for s in candidates:
        obj = _try_load(s)
        if isinstance(obj, dict):
            return obj
    return None


class ContractHoldingsORM(ORM):
    """Format/number contract for holdings output while allowing reasoning.

    - Extracts the first occurrence of {"holding_log_delta": <num>} from the <answer> body (tolerates extra text).
    - Numeric value must be finite (no NaN/inf) with up to 6 decimals.
    Reward: +1.0 if a valid value is found, else -1.0.
    """

    _LOG_DELTA_SEARCH = re.compile(r'"holding_log_delta"\s*:\s*(-?\d+(?:\.\d{1,6})?)', re.IGNORECASE)

    def __call__(self, completions, holding_t=None, **kwargs) -> List[float]:
        rewards: List[float] = []
        if not isinstance(holding_t, list):
            holding_t = [holding_t] * len(completions)
        for comp, ht in zip(completions, holding_t):
            try:
                text = comp or ""
                lower_text = text.lower()
                if "<answer>" not in lower_text or "</answer>" not in lower_text:
                    rewards.append(-1.0)
                    continue
                think_match = THINK_CONTENT_RE.search(text)
                if not think_match or not think_match.group(1).strip():
                    rewards.append(-1.0)
                    continue
                answer_pos = lower_text.find("<answer>")
                if answer_pos > -1:
                    prefix = text[:answer_pos]
                    cleaned_prefix = THINK_CONTENT_RE.sub("", prefix).strip()
                    if "assistant" in cleaned_prefix.lower() or "</answer>" in cleaned_prefix.lower():
                        rewards.append(-1.0)
                        continue

                # Accept values inside <answer> or anywhere in completion as fallback
                body = _extract_answer_body(comp) or comp or ""

                m = self._LOG_DELTA_SEARCH.search(body)
                if m:
                    val = float(m.group(1))
                    if not math.isfinite(val):
                        rewards.append(-1.0)
                        continue
                    rewards.append(1.0)
                    continue

                rewards.append(-1.0)
            except Exception:
                rewards.append(-1.0)
        return rewards


class HoldingsDeltaORM(ORM):
    """Composite reward = w_mag * R_mag + w_dir * R_dir.

    - Magnitude reward R_mag: normalized Huber on error e=pred-target with adaptive/robust scale.
    - Direction reward R_dir: sigmoid on scaled pred aligned with sign(target).
    Accepts holding_delta values only.
    """

    def __init__(self):
        self._ema_abs_err = None  # type: float | None
        self._ema_abs_r = None    # type: float | None

    @staticmethod
    def _sigmoid(x: float) -> float:
        x = max(min(x, 20.0), -20.0)
        return 1.0 / (1.0 + math.exp(-x))

    @staticmethod
    def _huber(e: float, c: float) -> float:
        ae = abs(e)
        if ae <= c:
            return 0.5 * e * e
        return c * (ae - 0.5 * c)

    @staticmethod
    def _robust_scale(vals: list[float], method: str, k: float, eps: float = 1e-8) -> float:
        v = [abs(x) for x in vals if x is not None]
        if not v:
            return eps
        v.sort()
        if method == 'iqr':
            q1 = v[int(0.25 * (len(v) - 1))]
            q3 = v[int(0.75 * (len(v) - 1))]
            iqr = max(q3 - q1, eps)
            return k * iqr
        # mad
        med = v[len(v) // 2]
        mad = [abs(x - med) for x in v]
        mad.sort()
        m = mad[len(mad) // 2]
        return k * max(m, eps)

    def __call__(self, completions, label_delta=None, label_tp1=None, holding_t=None, **kwargs) -> List[float]:
        k_mag = float(kwargs.get('k_mag', 1.5))
        k_dir = float(kwargs.get('k_dir', 1.0))
        ema_lambda = float(kwargs.get('ema_lambda', 0.9))
        alpha = float(kwargs.get('alpha', 5.0))
        margin = float(kwargs.get('margin', 0.0))
        w_mag = float(kwargs.get('w_mag', 0.6))
        w_dir = float(kwargs.get('w_dir', 0.4))
        robust_mode = str(kwargs.get('robust_mode', 'ema'))  # 'ema', 'mad', 'iqr'

        rewards: List[float] = []
        # allow holding_log_delta alias
        if label_delta is None and "holding_log_delta" in kwargs:
            label_delta = kwargs.get("holding_log_delta")
        if not isinstance(label_delta, list):
            label_delta = [label_delta] * len(completions)
        if not isinstance(label_tp1, list):
            label_tp1 = [label_tp1] * len(completions)
        if not isinstance(holding_t, list):
            holding_t = [holding_t] * len(completions)

        preds: list[float | None] = []
        targets: list[float | None] = []
        for comp, gt_delta, gt_tp1, ht in zip(completions, label_delta, label_tp1, holding_t):
            pred = None
            try:
                obj = _extract_json_from_answer(comp)
                if isinstance(obj, dict):
                    if obj.get('holding_log_delta') is not None:
                        pred = float(obj['holding_log_delta'])
                    elif obj.get('holding_delta') is not None:
                        pred = float(obj['holding_delta'])
            except Exception:
                pred = None

            tgt = None
            try:
                if gt_delta is not None:
                    tgt = float(gt_delta)
            except Exception:
                tgt = None

            preds.append(pred)
            targets.append(tgt)

        es: list[float] = []
        rs: list[float] = []
        for pred, tgt in zip(preds, targets):
            if pred is None or tgt is None:
                es.append(None)  # type: ignore
                rs.append(None)  # type: ignore
            else:
                es.append(pred - tgt)
                rs.append(tgt)

        eps = 1e-6
        if robust_mode == 'ema':
            def _mean_abs(vs):
                v = [abs(x) for x in vs if x is not None]
                return (sum(v) / len(v)) if v else None

            mean_abs_e = _mean_abs(es)
            mean_abs_r = _mean_abs(rs)
            if mean_abs_e is not None:
                self._ema_abs_err = (
                    float(mean_abs_e) if self._ema_abs_err is None else ema_lambda * float(self._ema_abs_err)
                    + (1 - ema_lambda) * float(mean_abs_e)
                )
            if mean_abs_r is not None:
                self._ema_abs_r = (
                    float(mean_abs_r) if self._ema_abs_r is None else ema_lambda * float(self._ema_abs_r)
                    + (1 - ema_lambda) * float(mean_abs_r)
                )
            c_mag = k_mag * float(self._ema_abs_err if self._ema_abs_err is not None else 1.0)
            c_dir = k_dir * float(self._ema_abs_r if self._ema_abs_r is not None else 1.0)
        else:
            method = 'mad' if robust_mode == 'mad' else 'iqr'
            c_mag = self._robust_scale([x for x in es if x is not None], method, k_mag, eps)
            c_dir = self._robust_scale([x for x in rs if x is not None], method, k_dir, eps)
        c_mag = max(c_mag, eps)
        c_dir = max(c_dir, eps)

        for pred, tgt, e, r_val in zip(preds, targets, es, rs):
            if pred is None or tgt is None or e is None or r_val is None:
                rewards.append(-1.0)
                continue
            sig = math.copysign(math.log1p(abs(e)), e)
            sim01 = 1.0 - min((sig * sig) / 4.0, 1.0)
            r_mag = 2.0 * sim01 - 1.0

            s = (pred / c_dir) * (1.0 if r_val >= 0 else -1.0)
            r_dir = self._sigmoid(alpha * (s - margin))
            rewards.append(float(w_mag * r_mag + w_dir * (2.0 * r_dir - 1.0)))

        return rewards


# register names for --reward_funcs
orms['external_holdings'] = HoldingsDeltaORM
orms['contract_holdings'] = ContractHoldingsORM


class DirectionHoldingsORM(ORM):
    """Reward measuring directional alignment with optional soft weighting."""

    def __call__(self, completions, label_delta=None, **kwargs) -> List[float]:
        if label_delta is None and "holding_log_delta" in kwargs:
            label_delta = kwargs.get("holding_log_delta")
        if not isinstance(label_delta, list):
            label_delta = [label_delta] * len(completions)

        rewards: List[float] = []
        tol = float(kwargs.get("direction_eps", 2e-2))
        scale = float(kwargs.get("sign_scale", 5.0))
        use_weight = bool(kwargs.get("sign_weighted", True))
        weight_cap = float(kwargs.get("sign_weight_cap", 0.2))

        for comp, tgt in zip(completions, label_delta):
            pred = None
            try:
                obj = _extract_json_from_answer(comp)
                if isinstance(obj, dict) and obj.get("holding_log_delta") is not None:
                    pred = float(obj["holding_log_delta"])
            except Exception:
                pred = None

            try:
                tgt_val = float(tgt) if tgt is not None else None
            except Exception:
                tgt_val = None

            if pred is None or tgt_val is None or not math.isfinite(pred) or not math.isfinite(tgt_val):
                rewards.append(-1.0)
                continue

            tgt_abs = abs(tgt_val)
            if tgt_abs < tol:
                rewards.append(0.0)
                continue

            soft = math.tanh(scale * (pred * tgt_val))
            if use_weight:
                weight = min(tgt_abs / weight_cap, 1.0) if weight_cap > 0 else 1.0
                soft *= weight

            rewards.append(float(soft))

        return rewards


orms["direction_holdings"] = DirectionHoldingsORM


class MagnitudeHoldingsORM(ORM):
    """Reward for magnitude closeness; maps absolute error to [-1, 1]."""

    def __call__(self, completions, label_delta=None, **kwargs) -> List[float]:
        if label_delta is None and "holding_log_delta" in kwargs:
            label_delta = kwargs.get("holding_log_delta")
        if not isinstance(label_delta, list):
            label_delta = [label_delta] * len(completions)

        threshold = float(kwargs.get("threshold", 0.2))
        threshold = max(threshold, 1e-6)

        rewards: List[float] = []
        for comp, tgt in zip(completions, label_delta):
            pred = None
            try:
                obj = _extract_json_from_answer(comp)
                if isinstance(obj, dict) and obj.get("holding_log_delta") is not None:
                    pred = float(obj["holding_log_delta"])
            except Exception:
                pred = None

            try:
                tgt_val = float(tgt) if tgt is not None else None
            except Exception:
                tgt_val = None

            if pred is None or tgt_val is None or not math.isfinite(pred) or not math.isfinite(tgt_val):
                rewards.append(-1.0)
                continue

            diff = abs(pred - tgt_val)
            ratio = min(diff / threshold, 1.0)
            r_mag = 1.0 - ratio
            rewards.append(2.0 * r_mag - 1.0)

        return rewards


orms["magnitude_holdings"] = MagnitudeHoldingsORM


class HuberHoldingsORM(ORM):
    """Reward based on Huber loss mapped linearly into [-1, 1]."""

    def __init__(self):
        self._ema_scale: float | None = None
        self._recent_errors: list[float] = []

    def __call__(self, completions, label_delta=None, **kwargs) -> List[float]:
        if not isinstance(label_delta, list):
            label_delta = [label_delta] * len(completions)

        delta = float(kwargs.get("delta", 0.05))
        base_cap = float(kwargs.get("huber_cap", 0.12))
        base_cap = max(base_cap, 1e-8)
        adaptive_mode = str(kwargs.get("adaptive_cap", "ema")).lower()
        ema_lambda = float(kwargs.get("ema_lambda", 0.9))
        cap_scale = float(kwargs.get("cap_scale", 2.0))
        cap_floor = float(kwargs.get("cap_floor", base_cap))
        cap_percentile = float(kwargs.get("cap_percentile", 90.0))
        cap_window = int(kwargs.get("cap_window", 512))

        rewards: List[float] = []
        errs: list[float | None] = []
        for comp, tgt in zip(completions, label_delta):
            pred = None
            try:
                obj = _extract_json_from_answer(comp)
                if isinstance(obj, dict) and obj.get("holding_log_delta") is not None:
                    pred = float(obj["holding_log_delta"])
            except Exception:
                pred = None

            try:
                tgt_val = float(tgt) if tgt is not None else None
            except Exception:
                tgt_val = None

            if pred is None or tgt_val is None or not math.isfinite(pred) or not math.isfinite(tgt_val):
                errs.append(None)
                continue

            errs.append(pred - tgt_val)

        valid_errs = [e for e in errs if e is not None]
        huber_cap = base_cap
        if adaptive_mode == "ema" and valid_errs:
            mean_abs = float(sum(abs(e) for e in valid_errs) / len(valid_errs))
            if self._ema_scale is None:
                self._ema_scale = mean_abs
            else:
                self._ema_scale = float(ema_lambda * self._ema_scale + (1 - ema_lambda) * mean_abs)
            huber_cap = max(cap_floor, float(self._ema_scale) * cap_scale)
        elif adaptive_mode == "percentile" and valid_errs:
            self._recent_errors.extend(abs(e) for e in valid_errs)
            if len(self._recent_errors) > cap_window:
                self._recent_errors = self._recent_errors[-cap_window:]
            if self._recent_errors:
                huber_cap = max(cap_floor, float(np.percentile(self._recent_errors, cap_percentile)))

        huber_cap = max(huber_cap, 1e-8)

        for e in errs:
            if e is None:
                rewards.append(-1.0)
                continue
            ae = abs(e)
            if ae <= delta:
                huber = 0.5 * (e ** 2)
            else:
                huber = delta * (ae - 0.5 * delta)
            ratio = min(huber / huber_cap, 1.0)
            rewards.append(1.0 - 2.0 * ratio)

        return rewards


orms["huber_holdings"] = HuberHoldingsORM


# ============================
# Profile–Reasoning Alignment Reward
# ============================
class ProfileNumericDeviationORM(ORM):
    """
    Numeric profile deviation penalty (penalty-only, tail-only).

    This reward enforces that predicted holding changes do NOT violate
    the investor-type profile in extreme ways, but NEVER rewards matching
    the profile mean.

    Profile definitions (consistent with profile construction):
      risk_aversion ~ |ΔlogH| / VIX
      herd_behavior ~ |ΔlogH| / ln(market_volume)

    Reward <= 0.0 (penalty-only).
    """

    def __call__(
        self,
        completions,
        objective_weights=None,
        vix_q_prev=None,
        ln_market_volume_q_prev=None,
        profile_threshold=None,
        **kwargs
    ) -> List[float]:
        """
        Args:
            completions: model outputs
            objective_weights: dict or list of dicts
                {
                  "risk_aversion": float,
                  "herd_behavior": float,
                  "profit_driven": float
                }

            vix_q_prev: float or list[float]
                Mean VIX over [t-1 quarter, t quarter)

            ln_market_volume_q_prev: float or list[float]
                Mean log market volume over same window

            profile_threshold: optional float or list[float]
                Precomputed tail threshold S_q90[type].
                If None, a conservative adaptive proxy is used.

        Returns:
            rewards: List[float] ≤ 0.0
        """

        # broadcast inputs
        if not isinstance(objective_weights, list):
            objective_weights = [objective_weights] * len(completions)
        if not isinstance(vix_q_prev, list):
            vix_q_prev = [vix_q_prev] * len(completions)
        if not isinstance(ln_market_volume_q_prev, list):
            ln_market_volume_q_prev = [ln_market_volume_q_prev] * len(completions)
        if profile_threshold is not None and not isinstance(profile_threshold, list):
            profile_threshold = [profile_threshold] * len(completions)

        rewards: List[float] = []

        eps = float(kwargs.get("eps", 1e-9))
        tol = float(kwargs.get("profile_tol", 0.35))   # band width
        scale = float(kwargs.get("penalty_scale", 1.0))

        for i, comp in enumerate(completions):
            try:
                # -------- extract prediction --------
                obj = _extract_json_from_answer(comp)
                if not isinstance(obj, dict) or obj.get("holding_log_delta") is None:
                    rewards.append(0.0)
                    continue

                delta_log = float(obj["holding_log_delta"])
                if not math.isfinite(delta_log):
                    rewards.append(0.0)
                    continue

                abs_delta = abs(delta_log)

                # -------- market state --------
                vix = max(abs(float(vix_q_prev[i])), eps)
                volm = max(abs(float(ln_market_volume_q_prev[i])), eps)

                # -------- profile weights --------
                ow = objective_weights[i] or {}
                w_risk = float(ow.get("risk_aversion", 0.0))
                w_herd = float(ow.get("herd_behavior", 0.0))
                # profit_driven deliberately excluded from numeric constraint

                # -------- intensity measures (same as profile script) --------
                risk_intensity = abs_delta / vix
                herd_intensity = abs_delta / volm

                # weighted intensity score
                S = w_risk * risk_intensity + w_herd * herd_intensity

                # -------- tail threshold --------
                if profile_threshold is not None:
                    base_th = float(profile_threshold[i])
                else:
                    # adaptive conservative proxy (safe default)
                    base_th = (w_risk / vix + w_herd / volm)

                upper = base_th * (1.0 + tol)

                # -------- tail-only hinge penalty --------
                if S <= upper:
                    rewards.append(0.0)
                else:
                    penalty = -scale * (S - upper) ** 2
                    rewards.append(float(penalty))

            except Exception:
                rewards.append(0.0)

        return rewards




orms["profile_numeric_deviation"] = ProfileNumericDeviationORM
